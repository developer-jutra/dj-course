ENGINE=GEMINI
GEMINI_API_KEY="ðŸ¥²-SIEMANKO-TU-API-KEY-ðŸ¥²"
MODEL_NAME="gemini-2.5-flash"

# ENGINE=LLAMA_CPP
# MODEL_NAME=llama-3.1-8b-instruct
# LLAMA_MODEL_PATH=/TWOJA_SCIEZKA_DO_MODELU/llama.cpp/NAZWA_MODELU.gguf
# LLAMA_GPU_LAYERS=1
# LLAMA_CONTEXT_SIZE=2048

# Ollama REST Client (using requests library and REST API directly)
# ENGINE=OLLAMA_REST
# MODEL_NAME=llama3.1:8b-instruct-q4_K_M
# OLLAMA_BASE_URL=http://localhost:11434
# OLLAMA_TIMEOUT=120

# Ollama Python SDK Client (using ollama-python library as an API wrapper)
# ENGINE=OLLAMA_PYTHON
# MODEL_NAME=llama3.1:8b-instruct-q8_0
# OLLAMA_HOST=http://localhost:11434
# OLLAMA_TIMEOUT=120.0

# General LLM Generation Parameters (optional, works with GEMINI and OLLAMA_REST)
# Controls the randomness and creativity of model responses
# TEMPERATURE=0.7       # Range: 0.0-2.0 (lower = more deterministic, higher = more creative; above 1.0 can produce unpredictable results)
# TOP_P=0.9             # Range: 0.0-1.0 (nucleus sampling threshold)
# TOP_K=40              # Integer â‰¥ 1 (limits token selection to top K tokens)
