# MLFlow quickstart

MLFlow to paczka pythonowa - musisz mieÄ‡ jÄ… zainstalowanÄ… (globalnie lub Å›rodowisko typu virtualenv).
Quickstart: https://mlflow.org/docs/latest/genai/getting-started/

JeÅ›li komuÅ› krwawiÄ… oczy na widok pythona to jest jeszcze API JS ðŸ˜… https://mlflow.org/docs/latest/genai/tracing/quickstart/typescript-openai/

## FLOW

- agent integruje siÄ™ z modelem (wiadomo)
- mlflow "nasÅ‚uchuje" tÄ™ komunikacjÄ™
- musimy sprowokowaÄ‡ dowolnego nasÅ‚uchiwanego agenta aby siÄ™ "odezwaÅ‚" do modelu
  - moÅ¼e to byÄ‡ `claude -p "bla bla"` (z CLI)
  - moÅ¼e to byÄ‡ dowolny lokalny model (uruchomiony za poÅ›rednictwem ollama, llama.cpp, cokolwiek)
- (po zainstalowaniu zaleÅ¼noÅ›ci - wiadomo) uruchamiasz `mlflow ui`

SzczegÃ³Å‚owe instrukcje poniÅ¼ej.

## venv setup

```bash
python -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt
```

albo `pip install mlflow`

## uruchomienie MLFlow UI (konieczne)

    mlflow ui
    mlflow ui --port 5001

## PodsÅ‚uchiwanie Claude

    mlflow autolog claude -u file:./mlruns

check:

    mlflow autolog claude --status

uruchomienie claude z konsoli:

    claude -p "write a poem about MLFlow in Polish"

albo edycja pliku pythonowego:

    claude -p "claude -p 'refactor the python function in test.py to use list comprehension'"

wyÅ‚Ä…czenie:

    mlflow autolog claude --disable

## lokalne modele

zobacz config w pliku `run-local-model.py`:
- czy uÅ¼ywasz llama.cpp, ollama, czy jeszcze czegoÅ› innego (byoe kompatybilnego z OpenAI API)
- stawianie lokalnego modelu:
  - ollama: `ollama run <MODEL>` np. `ollama run gemma3:27b`
  - llama.cpp: `llama-server -m <PATH>/<MODEL>` np. `llama-server -m ~/Library/Caches/llama.cpp/bartowski_Meta-Llama-3.1-8B-Instruct-GGUF_Meta-Llama-3.1-8B-Instruct-Q8_0.gguf`
    - lub to samo tylko `llama-cli` zamiast `llama-server`, np. `llama-cli -m ~/Library/Caches/llama.cpp/bartowski_Meta-Llama-3.1-8B-Instruct-GGUF_Meta-Llama-3.1-8B-Instruct-Q8_0.gguf`
- upewnij siÄ™, Å¼e `PORT` siÄ™ zgadza
- `ollama` wymaga poprawnej nazwy modelu (bo serwer ma ich wiele), zaÅ› dla `llama.cpp` - jej to obojÄ™tne
