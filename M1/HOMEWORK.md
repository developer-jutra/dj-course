# Zadanie 1

Skonfiguruj sobie dostp: Anthropic / Gemini / OpenAI.
Dostpne API: python / node.js, w sumie 6 r贸偶nych kombinacji:
- `external-model-anthropic-node`
- `external-model-anthropic-py`
- `external-model-google-genai-node`
- `external-model-google-genai-py`
- `external-model-openai-node`
- `external-model-openai-py`

Foldery zawieraj README z linkami do zakadania kont i kluczy API.
Mo偶na zasili model jednorazowo np. kwot 5$ i ustawi zmienne typu "maxTokens" na skrtajnie nisk warto typu 128 - w贸wsczas pojedynczy request kosztuje ~0.002$.
Google Gemini daje "hojne" darmowe quota na start.

W wybranym setupie stokenizuj odpowiedzi na wz贸r ![tokenized prompts and responses](./tokenized.png)

# Zadanie 2

Podepnij MLFlow - i podsuchaj, co agent robi z (jakim) modelem:
- Claude - najatwiej (1 komenda CLI)
- Gemini - z poziomu kody python
- Lokalne modele - z poziomu kodu python

zobacz `mlflow/README.md`

# Zadanie 3

Robimy wasny TOKENIZER.

Korpusy danych treningowych do wyboru:
- `korpus-nkjp`
- `korpus-wolnelektury`
- `korpus-spichlerz` (Bielik Team)
W repo znajdziesz instrukcje dla 3 r贸偶nych korpus贸w danych treningowych oraz bazowy kod pythonowy.

Zadania:
- stw贸rz wasne tokenizery w oparciu o plik `tokenizer-build.py` (obecna wersja dziaa ale jest zahardkodowana). Zdynamizuj kod w taki spos贸b, aby m贸c dynamicznie tworzy tokenizery w oparciu o zadane korpusy tekstowe. Stw贸rz
  - `tokenizer-pan-tadeusz.json` - tylko w oparciu o Pana Tadeusza ("wolnelektury")
  - `tokenizer-wolnelektury.json` - w oparciu o cay korpus "wolnelektury"
  - `tokenizer-nkjp.json` - w oparciu o cay korpus "nkjp"
  - `tokenizer-all-corpora.json` - w oparciu o wszystkie korpusy
- z HuggingFace wybierz LLM i cignij jego tokenizer (byle inny ni偶 Mistral-v0.1 - bo to ten sam co Bielik v0.1) i dodaj go do swoich tokenizer贸w
- w nawizaniu do sawnego badania ;) (https://arxiv.org/pdf/2503.01996) tokenizujemy r贸偶ne teksty "na krzy偶" r贸偶nymi tokenizerami
  - teksty:
    - "Pan Tadeusz, Ksiga 1" ("wolnelektury")
    - "The Pickwick Papers" (mini korpus / projekt gutenberg)
    - "Fryderyk Chopin" (mini korpus / wikipedia)
  - tokenizery - wszystkie dostpne (3 bielikowe + wybrany z HF + 4 stworzone)
  - zmontuj statystyki, kt贸re maj odpowiedzie na pytanie: **DLA KA呕DEGO TEKSTU, KTRY TOKENIZER BY NAJEFEKTYWNIEJSZY POD KTEM NAJMNIEJSZEJ ILOCI WYNIKOWYCH TOKENW?**
- sprawd藕 czy dla customowych tokenizer贸w zmiana rozmiaru sownika (default: `32k`) robi r贸偶nic na wyniki?

# Zadanie 4

Jeli skorzystasz z biblioteki `gensim`, mo偶e by konieczne zainstalowanie dodatkowych paczek lokalnie (W moim przypadku to byo `brew install gcc pkg-config openblas` oraz `export PKG_CONFIG_PATH="/opt/homebrew/opt/openblas/lib/pkgconfig"`)

a w zasadzie to si wycofaem z instalowania tego g贸wna przez homebrew bo to si nie koczy - i lec z `docker build -t scipy-env:latest .` i `docker run -it --rm -v "$(pwd)":/app scipy-env:latest /bin/bash`


# Zadanie 5

Zaimplementuj uproszczon wersj **ATTENTION SCORE MATRIX (S)**
Kod wyjciowy: folder `szczypta-machine-learning`.
Posikuj si ulubionym coding agent + deep research + discordem 

Plik: `src/homework.ts`
