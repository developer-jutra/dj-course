# Zadanie 5 - PyTorch Implementation

## üî• Implementacja Attention Score Matrix w PyTorch

### Instalacja PyTorch

#### Opcja 1: Automatyczna (zalecana)
```bash
# Windows + CUDA 12.6
pip3 install torch torchvision --index-url https://download.pytorch.org/whl/cu126

# Windows + CPU only
pip3 install torch torchvision --index-url https://download.pytorch.org/whl/cpu

# Linux / macOS
pip3 install torch torchvision
```

#### Opcja 2: Z pliku requirements
```bash
pip install -r requirements-pytorch.txt
```

**Sprawd≈∫ instalacjƒô:**
```bash
python -c "import torch; print(f'PyTorch {torch.__version__}')"
```

### Uruchomienie

```bash
cd M1/szczypta-machine-learning
python homework-pytorch.py
```

### Co robi skrypt?

1. **Wyja≈õnia podstawy PyTorch** - tensory, operacje, GPU
2. **Por√≥wnuje z NumPy** - pokazuje r√≥≈ºnice i podobie≈Ñstwa
3. **Testuje case-1** - pojedynczy przypadek testowy z szczeg√≥≈Çami
4. **Testuje wszystkie case'y** - case-1 do case-4 automatycznie

### Funkcje dostƒôpne

```python
from homework_pytorch import (
    calculate_attention_score_matrix,  # G≈Ç√≥wna funkcja obliczajƒÖca S
    run_single_test_case,              # Test pojedynczego case'u
    run_all_test_cases,                # Test wszystkich case'√≥w
    explain_pytorch_basics,            # Wyja≈õnienie PyTorch
    compare_with_numpy                 # Por√≥wnanie PyTorch vs NumPy
)

# Przyk≈Çad u≈ºycia
import torch

X = torch.tensor([[1.0, 2.0], [3.0, 4.0]])
W_Q = torch.tensor([[0.5, 0.5], [0.5, 0.5]])
W_K = torch.tensor([[1.0, 0.0], [0.0, 1.0]])

S = calculate_attention_score_matrix(X, W_Q, W_K)
print(S)
```

### Struktura plik√≥w

```
M1/szczypta-machine-learning/
‚îú‚îÄ‚îÄ homework-pytorch.py           # ‚Üê NOWY PLIK (implementacja PyTorch)
‚îú‚îÄ‚îÄ requirements-pytorch.txt      # ‚Üê NOWY PLIK (zale≈ºno≈õci)
‚îú‚îÄ‚îÄ README-pytorch.md             # ‚Üê NOWY PLIK (ta dokumentacja)
‚îú‚îÄ‚îÄ testcases/
‚îÇ   ‚îú‚îÄ‚îÄ case-1.json
‚îÇ   ‚îú‚îÄ‚îÄ case-2.json
‚îÇ   ‚îú‚îÄ‚îÄ case-3.json
‚îÇ   ‚îî‚îÄ‚îÄ case-4.json
‚îî‚îÄ‚îÄ src/
    ‚îú‚îÄ‚îÄ homework.ts               # Oryginalna implementacja TypeScript
    ‚îî‚îÄ‚îÄ ...
```

### Wz√≥r zaimplementowany

```
S = Q √ó K^T

gdzie:
  Q = X √ó W_Q    (Query Matrix)
  K = X √ó W_K    (Key Matrix)
```

### Operacje PyTorch

| Operacja | PyTorch | Opis |
|----------|---------|------|
| Tworzenie tensora | `torch.tensor(data)` | Konwersja z listy/array |
| Mno≈ºenie macierzy | `torch.matmul(A, B)` lub `A @ B` | Dot product |
| Transpozycja | `tensor.T` lub `.transpose(-2, -1)` | Zamiana wierszy z kolumnami |
| Wymiary | `tensor.shape` | Rozmiar tensora |
| Typ danych | `tensor.dtype` | float32, int64, etc. |
| UrzƒÖdzenie | `tensor.device` | CPU lub CUDA (GPU) |

### PyTorch vs NumPy

| Cecha | PyTorch | NumPy |
|-------|---------|-------|
| GPU Support | ‚úÖ `tensor.to('cuda')` | ‚ùå |
| Autograd | ‚úÖ Automatyczne gradienty | ‚ùå |
| Neural Networks | ‚úÖ `torch.nn` | ‚ùå |
| Sk≈Çadnia | Bardzo podobna | Bardzo podobna |
| Szybko≈õƒá (CPU) | Podobna | Podobna |
| Szybko≈õƒá (GPU) | üöÄ 10-100x szybciej | N/A |

### Przyk≈Çadowy output

```
KROK 1: Q = X √ó W_Q
  Wymiary: [2, 2] √ó [2, 2] = [2, 2]
  Q =
tensor([[3., 3.],
        [7., 7.]])

KROK 2: K = X √ó W_K
  Wymiary: [2, 2] √ó [2, 2] = [2, 2]
  K =
tensor([[1., 2.],
        [3., 4.]])

KROK 3: K^T (Transpozycja)
  Wymiary: [2, 2] ‚Üí [2, 2]
  K^T =
tensor([[1., 3.],
        [2., 4.]])

KROK 4: S = Q √ó K^T
  Wymiary: [2, 2] √ó [2, 2] = [2, 2]

üéØ ATTENTION SCORE MATRIX (S):
tensor([[ 9., 18.],
        [21., 42.]])
```

### Dodatkowe informacje

- **PyTorch Homepage**: https://pytorch.org/
- **Dokumentacja**: https://pytorch.org/docs/stable/index.html
- **Tutoriale**: https://pytorch.org/tutorials/
- **Quickstart**: https://pytorch.org/tutorials/beginner/basics/quickstart_tutorial.html
- **Tensors**: https://pytorch.org/tutorials/beginner/basics/tensorqs_tutorial.html

### GPU Support (opcjonalnie)

Je≈õli masz GPU NVIDIA z CUDA:

```python
# Sprawd≈∫ dostƒôpno≈õƒá CUDA
import torch
print(f"CUDA available: {torch.cuda.is_available()}")
print(f"GPU: {torch.cuda.get_device_name(0)}")

# Przenie≈õ obliczenia na GPU
X = torch.tensor([[1.0, 2.0]]).to('cuda')
W_Q = torch.tensor([[0.5], [0.5]]).to('cuda')
S = calculate_attention_score_matrix(X, W_Q, W_K)
```

### Troubleshooting

**Problem**: `ModuleNotFoundError: No module named 'torch'`
```bash
pip install torch torchvision
```

**Problem**: CUDA not available (mimo posiadania GPU)
```bash
# Reinstaluj z CUDA support
pip uninstall torch
pip install torch torchvision --index-url https://download.pytorch.org/whl/cu126
```

**Problem**: Brak pliku `case-X.json`
```bash
# Upewnij siƒô ≈ºe jeste≈õ w odpowiednim katalogu
cd M1/szczypta-machine-learning
ls testcases/  # powinno pokazaƒá case-1.json, case-2.json, etc.
```

### Zadanie wykonane ‚úÖ

- ‚úÖ Implementacja w PyTorch
- ‚úÖ Wszystkie 4 przypadki testowe
- ‚úÖ Szczeg√≥≈Çowe wyja≈õnienia
- ‚úÖ Por√≥wnanie z NumPy
- ‚úÖ Dokumentacja PyTorch basics
- ‚úÖ Interpretacja wynik√≥w
