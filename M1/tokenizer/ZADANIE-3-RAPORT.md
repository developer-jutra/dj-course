# Zadanie 3 - Tokenizer - Raport

## Podsumowanie Wykonanych Zada≈Ñ

### 1. Dynamizacja kodu tokenizer-build.py ‚úì

Zrefaktorowano oryginalny skrypt `tokenizer-build.py` aby umo≈ºliwiƒá:
- Dynamiczny wyb√≥r korpusu treningowego (PAN_TADEUSZ, WOLNELEKTURY, NKJP, ALL)
- Konfiguracjƒô rozmiaru s≈Çownika (vocab_size)
- Konfiguracjƒô minimalnej czƒôstotliwo≈õci (min_frequency)
- Automatyczne budowanie wszystkich tokenizer√≥w jednƒÖ komendƒÖ: `--all`

**Przyk≈Çady u≈ºycia:**
```bash
# Jeden tokenizer
python tokenizer-build.py --corpus PAN_TADEUSZ --output tokenizer-pan-tadeusz

# Wszystkie tokenizery naraz
python tokenizer-build.py --all

# Niestandardowy rozmiar s≈Çownika
python tokenizer-build.py --corpus WOLNELEKTURY --output my-tokenizer --vocab-size 16000
```

### 2. Stworzone Tokenizery ‚úì

Utworzono 4 wymagane tokenizery:

1. **tokenizer-pan-tadeusz.json** - trenowany tylko na Panu Tadeuszu (12 ksiƒÖg)
2. **tokenizer-wolnelektury.json** - trenowany na ca≈Çym korpusie Wolne Lektury (35 plik√≥w)
3. **tokenizer-nkjp.json** - trenowany na korpusie NKJP (3,889 plik√≥w)
4. **tokenizer-all-corpora.json** - trenowany na wszystkich dostƒôpnych korpusach (3,936 plik√≥w)

### 3. Tokenizer z HuggingFace ‚úì

Pobrany tokenizer: **gpt2-polish** (`sdadas/polish-gpt2-medium`)
- Stworzono skrypt `download-hf-tokenizer.py` do automatycznego pobierania tokenizer√≥w z HF
- Plik zapisany jako: `tokenizers/gpt2-polish.json`

### 4. Por√≥wnanie Krzy≈ºowe - Cross-Tokenization ‚úì

Utworzono skrypt `tokenizer-compare.py` kt√≥ry testuje wszystkie tokenizery na 3 tekstach referencyjnych:

**Teksty referencyjne:**
1. Pan Tadeusz - Ksiƒôga 1 (polski, klasyka)
2. The Pickwick Papers (angielski, klasyka)
3. Fryderyk Chopin (polski, Wikipedia)

**Dostƒôpne tokenizery (10 total):**
- 3x Bielik (v1, v2, v3)
- 1x GPT2-Polish (HuggingFace)
- 2x Existing (latarnik, mirrormid)
- 4x Custom (pan-tadeusz, wolnelektury, nkjp, all-corpora)

## Wyniki - Kt√≥ry Tokenizer Najefektywniejszy?

### Pan Tadeusz - Ksiƒôga 1 (43,734 znaki, ~6,845 s≈Ç√≥w)

| Ranking | Tokenizer | Token Count | Notatki |
|---------|-----------|-------------|---------|
| ü•á | **tokenizer-pan-tadeusz** | **9,985** | Trenowany na tym samym tek≈õcie |
| ü•à | tokenizer-all-corpora | 10,045 | -0.6% r√≥≈ºnicy |
| ü•à | tokenizer-wolnelektury | 10,045 | -0.6% r√≥≈ºnicy |
| 4 | latarnik_tokenizer | 10,045 | -0.6% r√≥≈ºnicy |
| 5 | tokenizer-nkjp | 11,066 | -9.8% gorszy |
| 6 | gpt2-polish | 11,908 | -16.2% gorszy |
| 7 | bielik-v3 | 13,177 | -24.2% gorszy |
| 8 | mirrormid | 17,292 | -42.2% gorszy |
| 9 | bielik-v2 | 20,480 | -51.2% gorszy |
| 10 | bielik-v1 | 20,481 | -51.2% gorszy |

**Wnioski:**
- Tokenizer trenowany na identycznym tek≈õcie (Pan Tadeusz) osiƒÖgnƒÖ≈Ç najlepszy wynik
- Tokenizery trenowane na polskich korpusach (wolnelektury, all-corpora) sƒÖ bardzo bliskie
- Bielik v3 znaczƒÖco lepszy od v1/v2 (32.4% mniej token√≥w!)
- Bielik v1/v2 (Mistral-based) bardzo nieefektywne dla polskiego tekstu

### The Pickwick Papers (1,746,334 znaki, ~300,090 s≈Ç√≥w)

| Ranking | Tokenizer | Token Count | Notatki |
|---------|-----------|-------------|---------|
| ü•á | **mirrormid_tokenizer** | **445,303** | Prawdopodobnie trenowany na angielskim |
| ü•à | bielik-v1 | 503,669 | +11.6% wiƒôcej |
| ü•à | bielik-v2 | 503,668 | +11.6% wiƒôcej |
| 4 | gpt2-polish | 713,919 | +37.6% wiƒôcej |
| 5 | tokenizer-nkjp | 725,407 | +38.6% wiƒôcej |
| 6 | bielik-v3 | 729,254 | +38.9% wiƒôcej |
| 7 | tokenizer-all-corpora | 750,535 | +40.7% wiƒôcej |
| 8 | tokenizer-wolnelektury | 824,016 | +46.0% wiƒôcej |
| 9 | latarnik_tokenizer | 824,016 | +46.0% wiƒôcej |
| 10 | tokenizer-pan-tadeusz | 926,941 | +51.9% wiƒôcej |

**Wnioski:**
- Tokenizer `mirrormid` radzi sobie najlepiej z angielskim tekstem
- Bielik v1/v2 zaskakujƒÖco dobre dla angielskiego (mimo polskiego focus)
- Customowe tokenizery polskie (pan-tadeusz, wolnelektury) s≈Çabe dla angielskiego
- Specjalizacja tokenizera ma ogromne znaczenie (52% r√≥≈ºnicy!)

### Fryderyk Chopin (59,585 znaki, ~8,251 s≈Ç√≥w)

| Ranking | Tokenizer | Token Count | Notatki |
|---------|-----------|-------------|---------|
| ü•á | **gpt2-polish** | **14,018** | Polski GPT2 z HF |
| ü•à | tokenizer-nkjp | 14,100 | +0.6% wiƒôcej |
| 3 | tokenizer-all-corpora | 14,732 | +4.8% wiƒôcej |
| 4 | bielik-v3 | 16,338 | +14.2% wiƒôcej |
| 5 | tokenizer-wolnelektury | 16,917 | +17.1% wiƒôcej |
| 6 | latarnik_tokenizer | 16,917 | +17.1% wiƒôcej |
| 7 | tokenizer-pan-tadeusz | 20,337 | +31.1% wiƒôcej |
| 8 | mirrormid | 22,438 | +37.5% wiƒôcej |
| 9 | bielik-v2 | 25,610 | +45.3% wiƒôcej |
| 10 | bielik-v1 | 25,611 | +45.3% g√≥rszy |

**Wnioski:**
- GPT2-Polish i NKJP najlepsze dla polskiego tekstu encyklopedycznego
- Tokenizer all-corpora te≈º bardzo dobry (tylko 4.8% gorszy)
- Bielik v3 >> v1/v2 (36% lepszy!)
- Pan Tadeusz tokenizer gorzej - zbyt specjalistyczny dla literatury XIX w.

## Eksperyment z Rozmiarami S≈Çownika (vocab_size)

Testowano rozmiary: 8k, 16k, 24k, 32k, 40k, 48k, 64k na korpusie PAN_TADEUSZ

**Wyniki:**

| Vocab Size | Token Count | vs. 32k Baseline |
|------------|-------------|------------------|
| 8,000 | 10,900 | +9.2% (gorsze) |
| 16,000 | 9,985 | 0.0% |
| 24,000 | 9,985 | 0.0% |
| 32,000 | 9,985 | 0.0% (baseline) |
| 40,000 | 9,985 | 0.0% |
| 48,000 | 9,985 | 0.0% |
| 64,000 | 9,985 | 0.0% |

**Wnioski:**
- Dla ma≈Çego korpusu (Pan Tadeusz) vocab_size > 16k nie daje poprawy
- Zbyt ma≈Çy vocab_size (8k) pogarsza wyniki o ~9%
- **Optymalny vocab_size dla Pan Tadeusz: 16,000**
- Korpus osiƒÖga "plateau" - ma tylko ~12,457 unikalnych par (merges)

### Interpretacja dla r√≥≈ºnych korpus√≥w:

1. **Ma≈Çe korpusy (Pan Tadeusz)**: vocab_size = 16k wystarczy
2. **≈örednie korpusy (Wolnelektury)**: vocab_size = 32k sensowny (31,882 merges)
3. **Du≈ºe korpusy (NKJP)**: vocab_size = 32k+ zalecany (31,832 merges osiƒÖgniƒôte)

## G≈Ç√≥wne Wnioski

### 1. Specjalizacja Jest Kluczowa
- Tokenizer trenowany na podobnym tek≈õcie (jƒôzyk, styl, epoka) daje najlepsze wyniki
- R√≥≈ºnica miƒôdzy najlepszym a najgorszym: **45-52%**

### 2. Ranking Og√≥lny (Polski Tekst)
1. ü•á **Custom tokenizer (dopasowany do tekstu)**
2. ü•à **GPT2-Polish** - uniwersalny, bardzo dobry
3. ü•â **Bielik v3** - du≈ºa poprawa vs v1/v2
4. **Tokenizer All-Corpora** - dobry kompromis
5. Bielik v1/v2 - s≈Çabe dla polskiego

### 3. Vocab Size
- Optymalizuj do rozmiaru korpusu
- Zbyt du≈ºy = niepotrzebny overhead
- Zbyt ma≈Çy = gorsze wyniki
- **Sweet spot: 16k-32k dla polskich tekst√≥w**

### 4. Tokenizacja dla Embeddingu (Zadanie 4)
Dla optymalnego embeddingu w zadaniu 4:
- U≈ºyj **tokenizer-pan-tadeusz** dla literatury polskiej
- U≈ºyj **gpt2-polish** dla og√≥lnych polskich tekst√≥w
- U≈ºyj **tokenizer-all-corpora** jako uniwersalny
- **Unikaj** Bielik v1/v2 - zbyt rozdrobnione tokeny

## Pliki Pomocnicze

### Skrypty
- `tokenizer-build.py` - budowanie tokenizer√≥w
- `tokenizer-compare.py` - por√≥wnanie krzy≈ºowe
- `test-vocab-sizes.py` - test r√≥≈ºnych vocab_size
- `download-hf-tokenizer.py` - pobieranie z HuggingFace

### Wyniki
- `tokenizer-comparison-results.json` - szczeg√≥≈Çowe wyniki w JSON

### Tokenizery (10 total)
```
tokenizers/
‚îú‚îÄ‚îÄ bielik-v1-tokenizer.json
‚îú‚îÄ‚îÄ bielik-v2-tokenizer.json
‚îú‚îÄ‚îÄ bielik-v3-tokenizer.json
‚îú‚îÄ‚îÄ gpt2-polish.json
‚îú‚îÄ‚îÄ latarnik_tokenizer.json
‚îú‚îÄ‚îÄ mirrormid_tokenizer.json
‚îú‚îÄ‚îÄ tokenizer-all-corpora.json
‚îú‚îÄ‚îÄ tokenizer-nkjp.json
‚îú‚îÄ‚îÄ tokenizer-pan-tadeusz.json
‚îî‚îÄ‚îÄ tokenizer-wolnelektury.json
```

## Rekomendacje

### Do Zadania 4 (Embedding)
1. U≈ºyj **tokenizer-pan-tadeusz** je≈õli pracujesz z literaturƒÖ polskƒÖ XIX w.
2. U≈ºyj **gpt2-polish** dla wsp√≥≈Çczesnego polskiego
3. U≈ºyj **tokenizer-all-corpora** jako uniwersalny baseline

### Og√≥lne
- Zawsze testuj sw√≥j tokenizer na reprezentatywnych tekstach
- Bielik v3 >> v1/v2 dla polskiego - aktualizuj je≈õli mo≈ºesz
- Vocab size dopasuj do wielko≈õci korpusu (nie zawsze wiƒôcej = lepiej)

---
**Data wykonania:** 2025-11-17
**≈örodowisko:** Python 3.11.9, tokenizers 0.22.1
