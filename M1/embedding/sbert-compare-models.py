"""
Por√≥wnanie modeli Sentence-BERT dla jƒôzyka polskiego.

Ten skrypt testuje r√≥≈ºne modele na tym samym korpusie i zapytaniach,
aby znale≈∫ƒá najlepszy model dla polskich tekst√≥w.

Testowane modele:
1. intfloat/multilingual-e5-small - wielojƒôzyczny (domy≈õlny)
2. sdadas/mmlw-retrieval-roberta-base - POLSKI ‚≠ê
3. sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2 - wielojƒôzyczny
4. sentence-transformers/LaBSE - wielojƒôzyczny Google

U≈ºycie:
    python sbert-compare-models.py
    python sbert-compare-models.py --corpus PAN_TADEUSZ  # szybszy test
"""

import numpy as np
import time
import argparse
from pathlib import Path
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
from corpora import CORPORA_FILES

# --- MODELE DO PRZETESTOWANIA ---

MODELS_TO_TEST = {
    "multilingual-e5-small": {
        "name": "intfloat/multilingual-e5-small",
        "description": "Wielojƒôzyczny E5 (domy≈õlny)",
        "size": "118M parametr√≥w",
        "languages": "100+ jƒôzyk√≥w"
    },
    "polish-roberta": {
        "name": "sdadas/mmlw-retrieval-roberta-base",
        "description": "Polski RoBERTa (NAJLEPSZY dla PL) ‚≠ê",
        "size": "~500MB",
        "languages": "Polski + wielojƒôzyczny"
    },
    "paraphrase-multilingual": {
        "name": "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",
        "description": "Paraphrase Multilingual",
        "size": "471M parametr√≥w",
        "languages": "50+ jƒôzyk√≥w"
    },
    "labse": {
        "name": "sentence-transformers/LaBSE",
        "description": "LaBSE (Language-agnostic BERT)",
        "size": "471M parametr√≥w",
        "languages": "109 jƒôzyk√≥w"
    }
}

# Zapytania testowe - polskie zdania z r√≥≈ºnych dziedzin
TEST_QUERIES = [
    "Kr√≥l wyda≈Ç rozkaz swoim rycerzom.",
    "Szlachta polska by≈Ça dumna ze swoich tradycji.",
    "Wojsko maszerowa≈Ço przez las w kierunku miasta.",
    "Mi≈Ço≈õƒá jest najwa≈ºniejsza w ≈ºyciu cz≈Çowieka.",
    "Lekarz zaleci≈Ç natychmiastowe leczenie choroby.",
    "Jestem bardzo g≈Çodny i chcia≈Çbym co≈õ zje≈õƒá.",
]

# --- FUNKCJE POMOCNICZE ---

def load_corpus(corpus_name="PAN_TADEUSZ", max_sentences=1000):
    """
    Wczytuje korpus tekstowy (ograniczona wersja dla szybko≈õci test√≥w).
    
    Args:
        corpus_name: Nazwa korpusu z CORPORA_FILES
        max_sentences: Maksymalna liczba zda≈Ñ (dla szybko≈õci)
    
    Returns:
        list: Lista zda≈Ñ
    """
    print(f"\nWczytywanie korpusu: {corpus_name}")
    print(f"Limit zda≈Ñ: {max_sentences}")
    
    files = CORPORA_FILES[corpus_name]
    sentences = []
    
    for file in files:
        try:
            with open(file, 'r', encoding='utf-8') as f:
                lines = [line.strip() for line in f if line.strip()]
                sentences.extend(lines)
                
                if len(sentences) >= max_sentences:
                    break
        except FileNotFoundError:
            continue
    
    sentences = sentences[:max_sentences]
    print(f"‚úì Wczytano {len(sentences):,} zda≈Ñ")
    
    return sentences

def test_model(model_key, model_info, corpus_sentences, test_queries):
    """
    Testuje pojedynczy model i zwraca metryki.
    
    Args:
        model_key: Klucz modelu (dla identyfikacji)
        model_info: Dict z informacjami o modelu
        corpus_sentences: Lista zda≈Ñ korpusu
        test_queries: Lista zapyta≈Ñ testowych
    
    Returns:
        dict: Metryki wydajno≈õci i jako≈õci
    """
    print(f"\n{'='*80}")
    print(f"TEST MODELU: {model_info['description']}")
    print(f"{'='*80}")
    print(f"Model: {model_info['name']}")
    print(f"Rozmiar: {model_info['size']}")
    print(f"Jƒôzyki: {model_info['languages']}")
    print(f"{'‚îÄ'*80}")
    
    results = {
        "model_key": model_key,
        "model_name": model_info['name'],
        "description": model_info['description']
    }
    
    # 1. ≈ÅADOWANIE MODELU
    print(f"\n1. ≈Åadowanie modelu...")
    try:
        start_load = time.time()
        model = SentenceTransformer(model_info['name'])
        load_time = time.time() - start_load
        print(f"   ‚úì Za≈Çadowano w {load_time:.2f}s")
        results['load_time'] = load_time
    except Exception as e:
        print(f"   ‚úó B≈ÅƒÑD: {e}")
        results['error'] = str(e)
        return results
    
    # 2. KODOWANIE KORPUSU
    print(f"\n2. Kodowanie korpusu ({len(corpus_sentences):,} zda≈Ñ)...")
    try:
        start_encode = time.time()
        corpus_embeddings = model.encode(
            corpus_sentences,
            batch_size=32,
            show_progress_bar=False,
            convert_to_numpy=True,
            normalize_embeddings=True
        )
        encode_time = time.time() - start_encode
        print(f"   ‚úì Zakodowano w {encode_time:.2f}s")
        print(f"   ≈örednio: {encode_time/len(corpus_sentences)*1000:.2f}ms/zdanie")
        results['encode_time'] = encode_time
        results['encode_speed'] = len(corpus_sentences) / encode_time
    except Exception as e:
        print(f"   ‚úó B≈ÅƒÑD: {e}")
        results['error'] = str(e)
        return results
    
    # 3. TESTOWANIE ZAPYTA≈É
    print(f"\n3. Testowanie zapyta≈Ñ ({len(test_queries)} zapyta≈Ñ)...")
    query_similarities = []
    
    for i, query in enumerate(test_queries, 1):
        # Zakoduj zapytanie
        query_embedding = model.encode(
            [query],
            convert_to_numpy=True,
            normalize_embeddings=True
        )
        
        # Oblicz podobie≈Ñstwo
        similarities = cosine_similarity(query_embedding, corpus_embeddings)[0]
        
        # Znajd≈∫ top-3
        top_3_indices = np.argsort(similarities)[::-1][:3]
        avg_top3_sim = np.mean([similarities[idx] for idx in top_3_indices])
        max_sim = similarities[top_3_indices[0]]
        
        query_similarities.append(avg_top3_sim)
        
        print(f"   Zapytanie {i}: max_sim={max_sim:.4f}, avg_top3={avg_top3_sim:.4f}")
        print(f"   ‚Üí \"{query[:60]}...\"")
        print(f"     Top wynik: \"{corpus_sentences[top_3_indices[0]][:60]}...\"")
    
    # Metryki jako≈õci
    avg_similarity = np.mean(query_similarities)
    min_similarity = np.min(query_similarities)
    max_similarity = np.max(query_similarities)
    
    results['avg_similarity'] = float(avg_similarity)
    results['min_similarity'] = float(min_similarity)
    results['max_similarity'] = float(max_similarity)
    results['std_similarity'] = float(np.std(query_similarities))
    
    print(f"\n   PODSUMOWANIE JAKO≈öCI:")
    print(f"   ‚Ä¢ ≈örednie podobie≈Ñstwo: {avg_similarity:.4f}")
    print(f"   ‚Ä¢ Min/Max: {min_similarity:.4f} / {max_similarity:.4f}")
    print(f"   ‚Ä¢ Odchylenie std: {results['std_similarity']:.4f}")
    
    return results

def print_comparison_table(all_results):
    """Wy≈õwietla tabelƒô por√≥wnawczƒÖ wszystkich modeli."""
    print(f"\n{'='*80}")
    print(f"POR√ìWNANIE MODELI - PODSUMOWANIE")
    print(f"{'='*80}\n")
    
    # Sortuj wed≈Çug ≈õredniego podobie≈Ñstwa (jako≈õƒá)
    valid_results = [r for r in all_results if 'error' not in r]
    sorted_by_quality = sorted(valid_results, key=lambda x: x['avg_similarity'], reverse=True)
    
    print(f"{'Rank':<6} {'Model':<35} {'Jako≈õƒá':<12} {'Czas [s]':<12} {'Szybko≈õƒá':<15}")
    print(f"{'‚îÄ'*100}")
    
    for rank, result in enumerate(sorted_by_quality, 1):
        emoji = "ü•á" if rank == 1 else "ü•à" if rank == 2 else "ü•â" if rank == 3 else "  "
        
        model_name = result['model_key']
        quality = result['avg_similarity']
        encode_time = result.get('encode_time', 0)
        speed = result.get('encode_speed', 0)
        
        print(f"{emoji} {rank:<4} {model_name:<35} {quality:<12.4f} {encode_time:<12.2f} {speed:<15.1f} zd/s")
    
    # Szczeg√≥≈Çowe por√≥wnanie
    print(f"\n{'='*80}")
    print(f"SZCZEG√ì≈ÅOWA ANALIZA")
    print(f"{'='*80}\n")
    
    for rank, result in enumerate(sorted_by_quality, 1):
        emoji = "ü•á" if rank == 1 else "ü•à" if rank == 2 else "ü•â" if rank == 3 else "‚óã"
        
        print(f"{emoji} {rank}. {result['description']}")
        print(f"   Model: {result['model_name']}")
        print(f"   Jako≈õƒá (avg similarity): {result['avg_similarity']:.4f}")
        print(f"   Czas kodowania: {result.get('encode_time', 0):.2f}s")
        print(f"   Szybko≈õƒá: {result.get('encode_speed', 0):.1f} zda≈Ñ/s")
        print(f"   Czas ≈Çadowania: {result.get('load_time', 0):.2f}s")
        print()
    
    # REKOMENDACJA
    print(f"{'='*80}")
    print(f"üéØ REKOMENDACJA")
    print(f"{'='*80}\n")
    
    best = sorted_by_quality[0]
    fastest = min(valid_results, key=lambda x: x.get('encode_time', float('inf')))
    
    print(f"‚ú® NAJLEPSZA JAKO≈öƒÜ: {best['description']}")
    print(f"   Model: {best['model_name']}")
    print(f"   Jako≈õƒá: {best['avg_similarity']:.4f}")
    print(f"   U≈ºyj: python sbert-encode-database.py --model {best['model_name']}")
    
    print(f"\n‚ö° NAJSZYBSZY: {fastest['description']}")
    print(f"   Model: {fastest['model_name']}")
    print(f"   Czas: {fastest['encode_time']:.2f}s")
    print(f"   Szybko≈õƒá: {fastest['encode_speed']:.1f} zda≈Ñ/s")
    
    # Trade-off
    if best['model_key'] != fastest['model_key']:
        quality_diff = (best['avg_similarity'] - fastest['avg_similarity']) / fastest['avg_similarity'] * 100
        time_diff = (fastest['encode_time'] - best['encode_time']) / best['encode_time'] * 100
        
        print(f"\n‚öñÔ∏è  TRADE-OFF:")
        print(f"   ‚Ä¢ Najlepszy model jest o {quality_diff:.1f}% lepszy jako≈õciowo")
        print(f"   ‚Ä¢ Ale o {abs(time_diff):.1f}% wolniejszy w kodowaniu")
    
    print(f"\n{'='*80}\n")

def main():
    """G≈Ç√≥wna funkcja."""
    parser = argparse.ArgumentParser(description='Por√≥wnanie modeli SBERT dla polskiego')
    parser.add_argument('--corpus', type=str, default='PAN_TADEUSZ',
                      choices=['PAN_TADEUSZ', 'WOLNELEKTURY', 'ALL'],
                      help='Korpus do testowania')
    parser.add_argument('--max-sentences', type=int, default=1000,
                      help='Maksymalna liczba zda≈Ñ z korpusu (dla szybko≈õci)')
    parser.add_argument('--models', type=str, nargs='+',
                      choices=list(MODELS_TO_TEST.keys()),
                      help='Wybierz konkretne modele do testowania')
    
    args = parser.parse_args()
    
    print(f"\n{'='*80}")
    print(f"POR√ìWNANIE MODELI SENTENCE-BERT DLA JƒòZYKA POLSKIEGO")
    print(f"{'='*80}")
    print(f"Korpus: {args.corpus}")
    print(f"Limit zda≈Ñ: {args.max_sentences}")
    print(f"Zapyta≈Ñ testowych: {len(TEST_QUERIES)}")
    print(f"{'='*80}")
    
    # Wczytaj korpus
    try:
        corpus = load_corpus(args.corpus, args.max_sentences)
    except Exception as e:
        print(f"\n‚úó B≈ÅƒÑD przy wczytywaniu korpusu: {e}")
        return
    
    # Wybierz modele do testowania
    if args.models:
        models_to_test = {k: v for k, v in MODELS_TO_TEST.items() if k in args.models}
    else:
        models_to_test = MODELS_TO_TEST
    
    print(f"\nLiczba modeli do przetestowania: {len(models_to_test)}")
    for key, info in models_to_test.items():
        print(f"  ‚Ä¢ {info['description']}")
    
    # Testuj ka≈ºdy model
    all_results = []
    
    for model_key, model_info in models_to_test.items():
        try:
            result = test_model(model_key, model_info, corpus, TEST_QUERIES)
            all_results.append(result)
        except Exception as e:
            print(f"\n‚úó B≈ÅƒÑD podczas testowania {model_key}: {e}")
            all_results.append({
                "model_key": model_key,
                "model_name": model_info['name'],
                "description": model_info['description'],
                "error": str(e)
            })
    
    # Wy≈õwietl por√≥wnanie
    if all_results:
        print_comparison_table(all_results)
    
    print(f"{'='*80}")
    print(f"‚úì TESTY ZAKO≈ÉCZONE")
    print(f"{'='*80}\n")

if __name__ == "__main__":
    main()
