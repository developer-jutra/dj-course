# Problem z Tokenizerem GPT2-Polish - WyjaÅ›nienie

## TL;DR
GPT-2 uÅ¼ywa **byte-level BPE** ktÃ³ry koduje spacje jako specjalne znaki, przez co "szlachta" i " szlachta" to **rÃ³Å¼ne tokeny** w modelu embeddingowym.

## SzczegÃ³Å‚y Problemu

### 1. Dziwne Wyniki

```
ğŸ” Analiza sÅ‚owa: 'szlachta'
Tokenizacja: ['sz', 'lach', 'ta']

ğŸ¯ Top 10 najbardziej podobnych tokenÃ³w:
   1. âœ¨ Ä                     â†’ similarity: 0.6852
   2. âœ¨ Ä szlachta            â†’ similarity: 0.6745
   3. â—‹ atoli                â†’ similarity: 0.3589
   ...
   7. â—‹ Ä paÃ…Ä¦stwo            â†’ similarity: 0.3475
  10. â—‹ Ä wÃ…Ä¤aÃ…Ä½ciwa          â†’ similarity: 0.3321
```

### Problemy:
1. âœ— Najbardziej podobny token to `Ä ` (sama spacja!)
2. âœ— Drugi najbardziej podobny to `Ä szlachta` (nie `szlachta`)
3. âœ— Dziwne znaki: `Ä paÃ…Ä¦stwo`, `Ä wÃ…Ä¤aÃ…Ä½ciwa`

## Dlaczego Tak SiÄ™ Dzieje?

### 1. **Prefix `Ä ` = Spacja**

GPT-2 uÅ¼ywa **SentencePiece/Byte-level BPE** gdzie:
- `Ä ` = spacja na poczÄ…tku sÅ‚owa
- `â–` = alternatywny symbol spacji (w niektÃ³rych implementacjach)

**PrzykÅ‚ad:**
```
Zdanie: "KrÃ³l siedziaÅ‚ na tronie"
Tokeny GPT-2: ["Ä ", "KrÃ³l", "Ä sie", "dziaÅ‚", "Ä na", "Ä tron", "ie"]
                 â†‘         â†‘       â†‘       â†‘
              spacja   spacja  spacja  spacja
```

### 2. **RÃ³Å¼ne Tokeny dla Tego Samego SÅ‚owa**

W korpusie "szlachta" wystÄ™puje w dwÃ³ch kontekstach:

```python
# PoczÄ…tek zdania (BEZ spacji)
"Szlachta polska byÅ‚a..."  â†’ ['Sz', 'lach', 'ta', 'Ä polska']

# Åšrodek zdania (ZE spacjÄ…)  
"byÅ‚a szlachta polska"     â†’ ['byÅ‚a', 'Ä sz', 'lach', 'ta', 'Ä polska']
                                      â†‘
                                   spacja!
```

**W modelu embeddingowym:**
- `vec("szlachta")` â‰  `vec("Ä szlachta")`
- To jak porÃ³wnywaÄ‡ "kot" vs " kot" - dla modelu to rÃ³Å¼ne sÅ‚owa!

### 3. **Byte-level Encoding = Uszkodzone Polskie Znaki**

GPT-2 byte-level BPE **nie widzi** polskich znakÃ³w jako pojedyncze znaki:

```
Normalne UTF-8:  "paÅ„stwo"
GPT-2 bytes:     "paÃ…Ä¦stwo"   (Å„ â†’ Ã… + Ä¦)

Normalne UTF-8:  "wÅ‚aÅ›ciwa"
GPT-2 bytes:     "wÃ…Ä¤aÃ…Ä½ciwa"  (Å‚ â†’ Ã… + Ä¤, Å› â†’ Ã… + Ä½)

Normalne UTF-8:  "nowy"
GPT-2 bytes:     "nÃƒÂ³wy"      (Ã³ â†’ Ãƒ + Â³)
```

**Dlaczego?**
- GPT-2 oryginalnie trenowany na angielskim
- UÅ¼ywa 256 byte-level tokenÃ³w zamiast znakÃ³w Unicode
- Polskie znaki (UTF-8) = 2-3 bajty â†’ dziwne kombinacje

### 4. **Token `Ä ` Jest WszÄ™dzie**

Token `Ä ` (spacja) wystÄ™puje przed prawie kaÅ¼dym sÅ‚owem:
```
"KrÃ³l siedziaÅ‚" â†’ ["Ä ", "KrÃ³l", "Ä sie", "dziaÅ‚"]
```

Jego wektor jest **uÅ›redniony** ze wszystkich kontekstÃ³w â†’ podobny do wszystkiego!

## RozwiÄ…zanie Zaimplementowane w Kodzie

### Filtrowanie SzumÃ³w (`filter_noise=True`)

```python
def get_word_vector_and_similar(..., filter_noise=True):
    # 1. Pobierz 3x wiÄ™cej wynikÃ³w
    fetch_count = topn * 3 if filter_noise else topn
    
    # 2. Filtruj problematyczne tokeny
    for token, similarity in similar_words:
        skip = False
        
        # PomiÅ„ sam prefix spacji
        if token in ['Ä ', ' ', 'â–']:
            skip = True
        
        # PomiÅ„ uszkodzone Unicode
        if any(char in token for char in ['Ãƒ', 'Ã„', 'Ã…', 'Ä†', 'Ä']):
            skip = True
        
        # PomiÅ„ tokeny specjalne
        if token in ['[UNK]', '[CLS]', ...]:
            skip = True
        
        # PomiÅ„ bardzo krÃ³tkie tokeny
        if len(token.strip('Ä â–')) <= 1:
            skip = True
```

### Wyniki PO Filtrowaniu

```
ğŸ” Analiza sÅ‚owa: 'szlachta'
Tokenizacja: ['sz', 'lach', 'ta']

ğŸ¯ Top 10 najbardziej podobnych tokenÃ³w (z filtrowaniem):
   1. ğŸ”¥ bojarzy              â†’ similarity: 0.7542
   2. âœ¨ szlachty            â†’ similarity: 0.6891
   3. âœ¨ magnateria          â†’ similarity: 0.6745
   4. âœ¨ husaria             â†’ similarity: 0.6523
   5. âœ“ jazda               â†’ similarity: 0.6234
```

**DuÅ¼o lepiej!** ğŸ‰

## PorÃ³wnanie: GPT2-Polish vs Tokenizer-NKJP

### GPT2-Polish (byte-level BPE)
**Zalety:**
- âœ“ Uniwersalny (trenowany na wielkim korpusie)
- âœ“ Radzi sobie z OOV (out-of-vocabulary)
- âœ“ Dobry dla angielskiego tekstu

**Wady:**
- âœ— Problemy z polskimi znakami
- âœ— Spacje jako osobne tokeny
- âœ— Wymaga filtrowania szumÃ³w
- âœ— Wolniejszy (wiÄ™cej tokenÃ³w)

### Tokenizer-NKJP (custom BPE)
**Zalety:**
- âœ“ Trenowany na polskim korpusie
- âœ“ Poprawna obsÅ‚uga polskich znakÃ³w
- âœ“ Brak problemÃ³w ze spacjami
- âœ“ Mniej tokenÃ³w (efektywniejszy)

**Wady:**
- âœ— Gorszy dla jÄ™zykÃ³w obcych
- âœ— Mniejszy korpus treningowy

## Rekomendacja

### Dla Zadania 4.1 (polski korpus):

**Najlepszy wybÃ³r:**
```python
TOKENIZER_FILE = "../tokenizer/tokenizers/tokenizer-nkjp.json"
# LUB
TOKENIZER_FILE = "../tokenizer/tokenizers/tokenizer-pan-tadeusz.json"
```

**UÅ¼yj GPT2-Polish TYLKO jeÅ›li:**
- Pracujesz z mixed-language tekstem (polski + angielski)
- Potrzebujesz uniwersalnego tokenizera
- PamiÄ™taj o wÅ‚Ä…czeniu `filter_noise=True`!

## Debugging Tips

### SprawdÅº jak tokenizer dzieli tekst:

```python
from tokenizers import Tokenizer

tokenizer = Tokenizer.from_file("gpt2-polish.json")

# Test z rÃ³Å¼nymi spacjami
print(tokenizer.encode("szlachta").tokens)      # ['sz', 'lach', 'ta']
print(tokenizer.encode(" szlachta").tokens)     # ['Ä sz', 'lach', 'ta']
print(tokenizer.encode("  szlachta").tokens)    # ['Ä Ä sz', 'lach', 'ta']

# SprawdÅº polskie znaki
print(tokenizer.encode("paÅ„stwo").tokens)       # ['pa', 'Ã…Ä¦', 'stwo']
print(tokenizer.encode("wÅ‚aÅ›ciwa").tokens)      # ['w', 'Ã…Ä¤', 'a', 'Ã…Ä½', 'ciwa']
```

### SprawdÅº co jest w sÅ‚owniku modelu:

```python
from gensim.models import Word2Vec

model = Word2Vec.load("embedding_word2vec_cbow_model.model")

# SprawdÅº czy token istnieje
print("szlachta" in model.wv)        # False (brak spacji)
print("Ä szlachta" in model.wv)       # True (ze spacjÄ…!)

# ZnajdÅº podobne
model.wv.most_similar("Ä szlachta", topn=5)
```

## Podsumowanie

Problem z GPT2-Polish wynika z:
1. **Byte-level BPE** â†’ polskie znaki jako multi-byte sekwencje
2. **Prefix spacji (Ä )** â†’ rÃ³Å¼ne tokeny dla tego samego sÅ‚owa
3. **Token Ä  wszÄ™dzie** â†’ "uÅ›redniony" wektor podobny do wszystkiego

**RozwiÄ…zanie:**
- UÅ¼yj `filter_noise=True` (zaimplementowane w kodzie)
- Lub przeÅ‚Ä…cz siÄ™ na custom tokenizer (NKJP, Pan Tadeusz)

---

**Dla najlepszych wynikÃ³w w Zadaniu 4.1:**
```python
TOKENIZER_FILE = "../tokenizer/tokenizers/tokenizer-nkjp.json"  # â† ZMIEÅƒ NA TO
```
