import numpy as np
import json
import logging
from gensim.models.doc2vec import Doc2Vec, TaggedDocument
from tokenizers import Tokenizer
import os
import glob
import time
from datetime import datetime
from pathlib import Path
from corpora import CORPORA_FILES

# Ustawienie logowania dla gensim
logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)

# --- KONFIGURACJA ---

# Rejestr wynik√≥w trening√≥w
TRAINING_REGISTRY_FILE = "doc2vec_training_registry.json"

# files = CORPORA_FILES["ALL"]
files = CORPORA_FILES["WOLNELEKTURY"]
# files = CORPORA_FILES["PAN_TADEUSZ"]

TOKENIZER_FILE = "../tokenizer/tokenizers/bielik-v3-tokenizer.json"
OUTPUT_MODEL_FILE = "doc2vec_model_combined.model"
OUTPUT_SENTENCE_MAP = "doc2vec_model_sentence_map_combined.json"

# Parametry treningu Doc2Vec
VECTOR_LENGTH = 20
WINDOW_SIZE = 6   
MIN_COUNT = 4         
WORKERS = 4           
EPOCHS = 20           
SG_MODE = 0   

# --- FUNKCJE POMOCNICZE ---

def load_training_registry():
    """
    Wczytuje rejestr wynik√≥w trening√≥w z pliku JSON.
    
    Returns:
        list: Lista s≈Çownik√≥w z historiƒÖ trening√≥w
    """
    if os.path.exists(TRAINING_REGISTRY_FILE):
        with open(TRAINING_REGISTRY_FILE, 'r', encoding='utf-8') as f:
            return json.load(f)
    return []

def save_training_registry(registry):
    """
    Zapisuje rejestr wynik√≥w trening√≥w do pliku JSON.
    
    Args:
        registry (list): Lista s≈Çownik√≥w z historiƒÖ trening√≥w
    """
    with open(TRAINING_REGISTRY_FILE, 'w', encoding='utf-8') as f:
        json.dump(registry, f, ensure_ascii=False, indent=2)

def calculate_embedding_quality(model, test_queries, sentence_lookup, tokenizer):
    """
    Oblicza jako≈õƒá embeddingu testujƒÖc na przyk≈Çadowych zapytaniach.
    
    Metryka: ≈õrednie podobie≈Ñstwo top-1 wyniku (wy≈ºsze = lepsze)
    
    Args:
        model: Wytrenowany model Doc2Vec
        test_queries (list): Lista testowych zda≈Ñ
        sentence_lookup (list): Lista wszystkich zda≈Ñ z korpusu
        tokenizer: Tokenizer do przetwarzania zda≈Ñ
        
    Returns:
        dict: S≈Çownik z metrykami jako≈õci
    """
    similarities = []
    
    for query in test_queries:
        tokens = tokenizer.encode(query).tokens
        inferred_vector = model.infer_vector(tokens, epochs=model.epochs)
        similar_docs = model.dv.most_similar([inferred_vector], topn=1)
        
        if similar_docs:
            similarities.append(similar_docs[0][1])  # similarity score
    
    avg_similarity = np.mean(similarities) if similarities else 0.0
    
    return {
        "avg_top1_similarity": float(avg_similarity),
        "test_queries_count": len(test_queries),
        "quality_rating": "excellent" if avg_similarity > 0.8 else 
                         "good" if avg_similarity > 0.6 else
                         "fair" if avg_similarity > 0.4 else "poor"
    }

def log_training_result(
    corpus_name,
    tokenizer_name,
    params,
    training_time,
    quality_metrics,
    corpus_stats
):
    """
    Loguje wyniki treningu do rejestru.
    
    Args:
        corpus_name (str): Nazwa korpusu
        tokenizer_name (str): Nazwa tokenizera
        params (dict): Parametry treningu
        training_time (float): Czas treningu w sekundach
        quality_metrics (dict): Metryki jako≈õci embeddingu
        corpus_stats (dict): Statystyki korpusu
    """
    # Wczytaj istniejƒÖcy rejestr
    registry = load_training_registry()
    
    # Utw√≥rz nowy wpis
    entry = {
        "run_id": len(registry) + 1,
        "timestamp": datetime.now().isoformat(),
        "corpus": {
            "name": corpus_name,
            "sentences_count": corpus_stats.get("sentences_count", 0),
            "avg_tokens_per_sentence": corpus_stats.get("avg_tokens", 0)
        },
        "tokenizer": tokenizer_name,
        "parameters": params,
        "training_time_seconds": round(training_time, 2),
        "quality_metrics": quality_metrics,
        "output_files": {
            "model": OUTPUT_MODEL_FILE,
            "sentence_map": OUTPUT_SENTENCE_MAP
        }
    }
    
    # Dodaj do rejestru
    registry.append(entry)
    
    # Zapisz
    save_training_registry(registry)
    
    print(f"\n{'='*80}")
    print(f"‚úì Wyniki treningu zapisane do rejestru")
    print(f"  Run ID: {entry['run_id']}")
    print(f"  Jako≈õƒá embeddingu: {quality_metrics['quality_rating'].upper()}")
    print(f"  Plik rejestru: {TRAINING_REGISTRY_FILE}")
    print(f"{'='*80}\n")   

# --- ETAP 1: Wczytanie, Tokenizacja i Przygotowanie Danych ---
try:
    tokenizer = Tokenizer.from_file(TOKENIZER_FILE)
except FileNotFoundError:
    print(f"B≈ÅƒÑD: Nie znaleziono pliku '{TOKENIZER_FILE}'. Upewnij siƒô, ≈ºe plik istnieje.")
    raise

# Wczytywanie i agregacja tekstu
raw_sentences = []
print("Wczytywanie tekstu z plik√≥w...")
print(f"Liczba plik√≥w do wczytania: {len(files)}") 

for file in files:
    try:
        with open(file, 'r', encoding='utf-8') as f:
            lines = [line.strip() for line in f if line.strip()] 
            raw_sentences.extend(lines)
    except FileNotFoundError:
        print(f"OSTRZE≈ªENIE: Nie znaleziono pliku '{file}'. Pomijam.")
        continue
    except Exception as e:
        print(f"B≈ÅƒÑD podczas przetwarzania pliku '{file}': {e}")
        continue

if not raw_sentences:
    print("B≈ÅƒÑD: Korpus danych jest pusty.")
    raise ValueError("Korpus danych jest pusty.")
print(f"Tokenizacja {len(raw_sentences)} zda≈Ñ...")

# Konwersja na listƒô token√≥w
tokenized_sentences = [
    tokenizer.encode(sentence).tokens for sentence in raw_sentences
]

# Statystyki korpusu
total_tokens = sum(len(sent) for sent in tokenized_sentences)
avg_tokens = total_tokens / len(tokenized_sentences) if tokenized_sentences else 0

corpus_stats = {
    "sentences_count": len(tokenized_sentences),
    "total_tokens": total_tokens,
    "avg_tokens": round(avg_tokens, 2)
}

print(f"Statystyki korpusu:")
print(f"  Zda≈Ñ: {corpus_stats['sentences_count']:,}")
print(f"  Token√≥w: {corpus_stats['total_tokens']:,}")
print(f"  ≈örednio token√≥w/zdanie: {corpus_stats['avg_tokens']:.1f}")

# Przygotowanie danych dla Doc2Vec
tagged_data = [
    TaggedDocument(words=tokenized_sentences[i], tags=[str(i)])
    for i in range(len(tokenized_sentences))
]
print(f"Przygotowano {len(tagged_data)} sekwencji TaggedDocument do treningu.")

# --- ETAP 2: Trening Doc2Vec ---
print(f"\n{'='*80}")
print(f"ETAP 2: Trening modelu Doc2Vec")
print(f"{'='*80}")
print(f"Parametry treningu:")
print(f"  ‚Ä¢ Wymiar wektora (vector_size): {VECTOR_LENGTH}")
print(f"  ‚Ä¢ Rozmiar okna (window): {WINDOW_SIZE}")
print(f"  ‚Ä¢ Min. czƒôsto≈õƒá (min_count): {MIN_COUNT}")
print(f"  ‚Ä¢ Liczba epok (epochs): {EPOCHS}")
print(f"  ‚Ä¢ Tryb: {'PV-DM (Distributed Memory)' if SG_MODE == 0 else 'PV-DBOW'}")
print(f"  ‚Ä¢ WƒÖtki (workers): {WORKERS}")
print(f"\nUruchamiam trening...")
print(f"{'='*80}\n")

start_time = time.time()
model_d2v = Doc2Vec(
    tagged_data,
    vector_size=VECTOR_LENGTH,
    window=WINDOW_SIZE,
    min_count=MIN_COUNT,
    workers=WORKERS,
    epochs=EPOCHS,
    dm=1 # Distributed Memory (PV-DM)
)
end_time = time.time()
training_time = end_time - start_time

print(f"\n{'='*80}")
print(f"‚úì Trening zako≈Ñczony pomy≈õlnie!")
print(f"  Czas treningu: {training_time:.2f}s ({training_time/60:.1f} min)")
print(f"{'='*80}")

# --- ETAP 3: Zapisywanie Wytrenowanego Modelu i Mapy ---
try:
    model_d2v.save(OUTPUT_MODEL_FILE)
    print(f"\nPe≈Çny model Doc2Vec zapisany jako: '{OUTPUT_MODEL_FILE}'.")
    
    with open(OUTPUT_SENTENCE_MAP, "w", encoding="utf-8") as f:
        json.dump(raw_sentences, f, ensure_ascii=False, indent=4)
    print(f"Mapa zda≈Ñ do ID zapisana jako: '{OUTPUT_SENTENCE_MAP}'.")

except Exception as e:
    # W kontek≈õcie 'po≈ÇƒÖczonego skryptu' b≈ÇƒÖd zapisu nie przerywa wnioskowania
    print(f"OSTRZE≈ªENIE: B≈ÅƒÑD podczas zapisu modelu/mapy: {e}. Kontynuujƒô wnioskowanie in-memory.")


# =========================================================================
# === ETAP 4: OCENA JAKO≈öCI EMBEDDINGU ===
# =========================================================================

print(f"\n{'='*80}")
print(f"ETAP 4: Ocena jako≈õci embeddingu")
print(f"{'='*80}\n")

# Testowe zapytania do oceny jako≈õci
test_queries = [
    "Jestem g≈Çodny i bardzo chƒôtnie zjad≈Çbym co≈õ.",
    "Kr√≥l siedzia≈Ç na tronie.",
    "Szlachta polska by≈Ça dumna ze swoich tradycji.",
    "Wojsko maszerowa≈Ço przez las.",
    "Mi≈Ço≈õƒá jest najwa≈ºniejsza w ≈ºyciu."
]

print(f"Testowanie na {len(test_queries)} przyk≈Çadowych zapytaniach...")
quality_metrics = calculate_embedding_quality(
    model_d2v, 
    test_queries, 
    raw_sentences, 
    tokenizer
)

print(f"\nMetryki jako≈õci:")
print(f"  ‚Ä¢ ≈örednie podobie≈Ñstwo top-1: {quality_metrics['avg_top1_similarity']:.4f}")
print(f"  ‚Ä¢ Ocena jako≈õci: {quality_metrics['quality_rating'].upper()}")
print(f"  ‚Ä¢ Liczba test√≥w: {quality_metrics['test_queries_count']}")

# =========================================================================
# === ETAP 5: LOGOWANIE WYNIK√ìW DO REJESTRU ===
# =========================================================================

print(f"\n{'='*80}")
print(f"ETAP 5: Zapisywanie wynik√≥w do rejestru")
print(f"{'='*80}\n")

# Przygotuj parametry do zapisu
training_params = {
    "VECTOR_LENGTH": VECTOR_LENGTH,
    "WINDOW_SIZE": WINDOW_SIZE,
    "MIN_COUNT": MIN_COUNT,
    "WORKERS": WORKERS,
    "EPOCHS": EPOCHS,
    "SG_MODE": SG_MODE,
    "dm": 1  # PV-DM
}

# Okre≈õl nazwƒô korpusu
corpus_name = "WOLNELEKTURY"  # Zmie≈Ñ dynamicznie je≈õli potrzeba
if files == CORPORA_FILES.get("ALL"):
    corpus_name = "ALL"
elif files == CORPORA_FILES.get("PAN_TADEUSZ"):
    corpus_name = "PAN_TADEUSZ"

# Okre≈õl nazwƒô tokenizera
tokenizer_name = Path(TOKENIZER_FILE).stem

# Zapisz wyniki do rejestru
log_training_result(
    corpus_name=corpus_name,
    tokenizer_name=tokenizer_name,
    params=training_params,
    training_time=training_time,
    quality_metrics=quality_metrics,
    corpus_stats=corpus_stats
)

# =========================================================================
# === ETAP 6: DEMONSTRACJA WNIOSKOWANIA (INFERENCE) ===
# =========================================================================

print(f"\n{'='*80}")
print(f"ETAP 6: Demonstracja wnioskowania")
print(f"{'='*80}\n")

#  üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•testowanieüî•üî•üî•üî•üî•üî•üî•üî•
demonstration_sentence = "Jestem g≈Çodny i bardzo chƒôtnie zjad≈Çbym co≈õ."
print(f"Zdanie do wnioskowania: \"{demonstration_sentence}\"")


# U≈ºywamy obiekt√≥w ju≈º za≈Çadowanych/wytrenowanych: model_d2v, tokenizer, raw_sentences
loaded_model = model_d2v # U≈ºywamy modelu prosto z treningu
sentence_lookup = raw_sentences # U≈ºywamy listy zda≈Ñ prosto z wczytywania korpusu


# Tokenizacja nowego zdania
new_tokens = tokenizer.encode(demonstration_sentence).tokens

# 2. Generowanie wektora dla nowego zdania
inferred_vector = loaded_model.infer_vector(new_tokens, epochs=loaded_model.epochs) 
print(f"\nWygenerowany wektor (embedding) dla zdania. Kszta≈Çt: {inferred_vector.shape}")

# 3. Znajdowanie najbardziej podobnych wektor√≥w z przestrzeni dokument√≥w/zda≈Ñ
# topn - liczba najbardziej podobnych zda≈Ñ do zwr√≥cenia
most_similar_docs = loaded_model.dv.most_similar([inferred_vector], topn=5)

print(f"\n{'‚îÄ'*80}")
print(f"üéØ Top 5 najbardziej podobnych zda≈Ñ z korpusu:")
print(f"{'‚îÄ'*80}")
for rank, (doc_id_str, similarity) in enumerate(most_similar_docs, 1):
    # 1. Konwertujemy ID (string) z powrotem na indeks (int)
    doc_index = int(doc_id_str)
    
    # 2. U≈ºywamy indeksu do odnalezienia oryginalnego tekstu
    try:
        original_sentence = sentence_lookup[doc_index]
        
        # Emoji dla poziom√≥w podobie≈Ñstwa
        if similarity > 0.8:
            emoji = "üî•"
        elif similarity > 0.6:
            emoji = "‚ú®"
        elif similarity > 0.4:
            emoji = "‚úì"
        else:
            emoji = "‚óã"
            
        print(f"  {rank}. {emoji} Podobie≈Ñstwo: {similarity:.4f}")
        print(f"     ID: {doc_id_str}")
        print(f"     Zdanie: {original_sentence}")
        print()
    except IndexError:
         print(f"  {rank}. ‚úó B≈ÅƒÑD: Nie znaleziono zdania dla ID: {doc_id_str}")
         print()

print(f"{'='*80}")
print(f"‚úì ZADANIE 4.2 UKO≈ÉCZONE")
print(f"{'='*80}")
print(f"\nPliki wyj≈õciowe:")
print(f"  ‚Ä¢ Model: {OUTPUT_MODEL_FILE}")
print(f"  ‚Ä¢ Mapa zda≈Ñ: {OUTPUT_SENTENCE_MAP}")
print(f"  ‚Ä¢ Rejestr trening√≥w: {TRAINING_REGISTRY_FILE}")
print(f"\nNastƒôpny krok:")
print(f"  Eksperymentuj z parametrami aby poprawiƒá jako≈õƒá embeddingu!")
print(f"  Sprawd≈∫ rejestr trening√≥w: cat {TRAINING_REGISTRY_FILE}")
print(f"{'='*80}")
