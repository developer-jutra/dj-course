"""
ZADANIE 4.1 - CBOW (Continuous Bag-of-Words) Embedding

Ten skrypt implementuje model embeddingowy CBOW, kt√≥ry uczy siƒô przewidywaƒá 
s≈Çowo docelowe (≈õrodkowe) na podstawie jego s≈Ç√≥w kontekstowych (otaczajƒÖcych).

G≈Å√ìWNE KROKI:
1. Za≈Çadowanie tokenizera BPE
2. Wczytanie i tokenizacja korpusu tekstowego
3. Trening modelu Word2Vec w trybie CBOW
4. Eksport wektor√≥w embeddingowych
5. Testowanie podobie≈Ñstwa semantycznego s≈Ç√≥w

CBOW vs Skip-gram:
- CBOW: kontekst ‚Üí s≈Çowo ≈õrodkowe (szybszy, lepszy dla czƒôstych s≈Ç√≥w)
- Skip-gram: s≈Çowo ≈õrodkowe ‚Üí kontekst (lepszy dla rzadkich s≈Ç√≥w)
"""

import numpy as np
import json
import logging
from gensim.models import Word2Vec
from tokenizers import Tokenizer
import os
import glob
from corpora import CORPORA_FILES # type: ignore 

# Ustawienie szczeg√≥≈Çowego logowania dla monitorowania procesu treningu
logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)

# --- KONFIGURACJA ≈öCIE≈ªEK I PARAMETR√ìW ---

# KROK 1: Wyb√≥r korpusu treningowego
# Dostƒôpne opcje: "WOLNELEKTURY", "PAN_TADEUSZ", "NKJP", "ALL"
# Wiƒôkszy korpus = lepsze embeddingi, ale d≈Çu≈ºszy trening
# files = CORPORA_FILES["WOLNELEKTURY"]  # ~35 plik√≥w, literatura polska
# files = CORPORA_FILES["PAN_TADEUSZ"]   # ~12 plik√≥w, tylko Pan Tadeusz
files = CORPORA_FILES["ALL"]             # ~3936 plik√≥w, wszystkie korpusy

# KROK 2: Wyb√≥r tokenizera
# Tokenizer dzieli tekst na podwyrazowe tokeny (np. "wojsko" ‚Üí ["woj", "sko"])
# Wyb√≥r tokenizera ma OGROMNY wp≈Çyw na jako≈õƒá embeddingu!
# Z Zadania 3 wiemy ≈ºe:
# - tokenizer-nkjp: dobry dla og√≥lnego polskiego (14,100 token√≥w dla Chopina)
# - tokenizer-pan-tadeusz: najlepszy dla literatury (9,985 token√≥w)
# - bielik-v3: lepszy od v1/v2 (13,177 token√≥w)
# - gpt2-polish: bardzo dobry uniwersalny (14,018 token√≥w)
TOKENIZER_FILE = "../tokenizer/tokenizers/tokenizer-nkjp.json"
# TOKENIZER_FILE = "../tokenizer/tokenizers/gpt2-polish.json"
# TOKENIZER_FILE = "../tokenizer/tokenizers/tokenizer-pan-tadeusz.json"
# TOKENIZER_FILE = "../tokenizer/tokenizers/bielik-v1-tokenizer.json"
# TOKENIZER_FILE = "../tokenizer/tokenizers/bielik-v3-tokenizer.json"

# KROK 3: Pliki wyj≈õciowe
OUTPUT_TENSOR_FILE = "embedding_tensor_cbow.npy"            # Macierz wektor√≥w (numpy array)
OUTPUT_MAP_FILE = "embedding_token_to_index_map.json"        # Mapowanie token‚Üíindeks
OUTPUT_MODEL_FILE = "embedding_word2vec_cbow_model.model"    # Pe≈Çny model gensim

# KROK 4: Parametry treningu Word2Vec (CBOW)
# Te parametry KRYTYCZNIE wp≈ÇywajƒÖ na jako≈õƒá i czas treningu!

VECTOR_LENGTH = 100   # Wymiar wektora embeddingowego (50-300 typowo)
                      # Wiƒôkszy = wiƒôcej informacji, ale wolniejszy trening
                      # Zalecane: 100-200 dla dobrych wynik√≥w

WINDOW_SIZE = 8      # Rozmiar okna kontekstowego (ile s≈Ç√≥w po ka≈ºdej stronie)
                      # CBOW u≈ºywa WINDOW_SIZE s≈Ç√≥w z lewej i prawej do przewidywania ≈õrodkowego
                      # Np. dla WINDOW_SIZE=2: [w1, w2, TARGET, w4, w5]
                      # Wiƒôkszy = wiƒôcej kontekstu, ale wolniejszy
                      # Zalecane: 5-10

MIN_COUNT = 2         # Minimalna czƒôsto≈õƒá wystƒôpowania tokenu
                      # Tokeny wystƒôpujƒÖce rzadziej ni≈º MIN_COUNT sƒÖ ignorowane
                      # Wiƒôkszy = mniej token√≥w, szybszy trening, ale gorsza pokrycie
                      # Zalecane: 2-5

WORKERS = 4           # Liczba wƒÖtk√≥w do r√≥wnoleg≈Çego treningu
                      # Ustaw na liczbƒô rdzeni CPU (sprawd≈∫: os.cpu_count())

EPOCHS = 25       # Liczba przej≈õƒá przez ca≈Çy korpus
                      # Wiƒôcej = lepsze wyniki, ale d≈Çu≈ºszy trening
                      # Zalecane: 10-50
                      # UWAGA: Zbyt wiele epok mo≈ºe prowadziƒá do overfittingu!

SAMPLE_RATE = 1e-3    # Wsp√≥≈Çczynnik downsamplingu dla czƒôstych s≈Ç√≥w
                      # Redukuje wp≈Çyw bardzo czƒôstych s≈Ç√≥w (np. "i", "w", "na")
                      # Typowo: 1e-3 do 1e-5
                      # 0 = wy≈ÇƒÖczone

SG_MODE = 0           # Tryb algorytmu: 0 = CBOW, 1 = Skip-gram
                      # CBOW: szybszy, lepszy dla czƒôstych s≈Ç√≥w
                      # Skip-gram: wolniejszy, lepszy dla rzadkich s≈Ç√≥w

# KROK 5: Za≈Çadowanie tokenizera
# Tokenizer BPE (Byte Pair Encoding) zosta≈Ç wytrenowany w Zadaniu 3
# ≈Åadujemy go z pliku JSON
try:
    print(f"\n{'='*80}")
    print(f"KROK 1: ≈Åadowanie tokenizera")
    print(f"{'='*80}")
    print(f"Plik tokenizera: {TOKENIZER_FILE}")
    tokenizer = Tokenizer.from_file(TOKENIZER_FILE)
    print(f"‚úì Tokenizer za≈Çadowany pomy≈õlnie")
    print(f"  Rozmiar s≈Çownika: {tokenizer.get_vocab_size()} token√≥w")
    
    # Wykryj typ tokenizera (GPT-2 style vs standardowy)
    # GPT-2 u≈ºywa 'ƒ†' jako prefix dla spacji
    IS_GPT2_STYLE = 'ƒ†' in tokenizer.get_vocab()
    if IS_GPT2_STYLE:
        print(f"  ‚ö† Wykryto tokenizer GPT-2 style (u≈ºywa prefiksu 'ƒ†' dla spacji)")
        print(f"  ‚Üí Zostanie zastosowana specjalna obs≈Çuga token√≥w")
    
except FileNotFoundError:
    print(f"‚úó B≈ÅƒÑD: Nie znaleziono pliku '{TOKENIZER_FILE}'.")
    print(f"  Upewnij siƒô, ≈ºe plik istnieje i ≈õcie≈ºka jest poprawna.")
    print(f"  Uruchom najpierw Zadanie 3 aby stworzyƒá tokenizery!")
    raise

# KROK 6: Funkcja agregacji zda≈Ñ z plik√≥w korpusu
def aggregate_raw_sentences(files):
    """
    Wczytuje i agreguje wszystkie zdania z plik√≥w korpusu.
    
    Proces:
    1. Iteruje przez ka≈ºdy plik w li≈õcie
    2. Wczytuje plik linia po linii (ka≈ºda linia = jedno zdanie)
    3. Usuwa puste linie i bia≈Çe znaki
    4. Dodaje wszystkie zdania do jednej listy
    
    Args:
        files (list): Lista ≈õcie≈ºek do plik√≥w tekstowych
        
    Returns:
        list[str]: Lista wszystkich zda≈Ñ ze wszystkich plik√≥w
        
    Uwagi:
        - Pliki muszƒÖ byƒá w kodowaniu UTF-8
        - Ka≈ºda linia w pliku traktowana jest jako osobne zdanie
        - Puste linie sƒÖ pomijane
        - Je≈õli plik nie istnieje, wy≈õwietla ostrze≈ºenie i kontynuuje
    """
    raw_sentences = []
    print(f"\n{'='*80}")
    print(f"KROK 2: Wczytywanie korpusu tekstowego")
    print(f"{'='*80}")
    print(f"Liczba plik√≥w do wczytania: {len(files)}")
    
    files_loaded = 0
    files_skipped = 0
    
    for file in files:
        try:
            with open(file, 'r', encoding='utf-8') as f:
                # Wczytaj wszystkie niepuste linie
                lines = [line.strip() for line in f if line.strip()]
                raw_sentences.extend(lines)
                files_loaded += 1
                
                # Wy≈õwietl progress co 500 plik√≥w
                if files_loaded % 500 == 0:
                    print(f"  Wczytano {files_loaded}/{len(files)} plik√≥w...")
                    
        except FileNotFoundError:
            print(f"‚ö† OSTRZE≈ªENIE: Nie znaleziono pliku '{file}'. Pomijam.")
            files_skipped += 1
            continue
        except Exception as e:
            print(f"‚ö† OSTRZE≈ªENIE: B≈ÇƒÖd przy wczytywaniu '{file}': {e}")
            files_skipped += 1
            continue

    print(f"\n‚úì Wczytano {files_loaded} plik√≥w")
    if files_skipped > 0:
        print(f"‚ö† Pominiƒôto {files_skipped} plik√≥w (b≈Çƒôdy)")
    print(f"‚úì Zebrano {len(raw_sentences):,} zda≈Ñ")
    
    if not raw_sentences:
        print("‚úó B≈ÅƒÑD: Pliki wej≈õciowe sƒÖ puste lub nie zosta≈Çy wczytane.")
        exit()
        
    return raw_sentences

raw_sentences = aggregate_raw_sentences(files)

# KROK 7: Tokenizacja korpusu
# Tokenizacja batch'owa - przetwarzamy wszystkie zdania naraz (wydajniej ni≈º jedno po jednym)
print(f"\n{'='*80}")
print(f"KROK 3: Tokenizacja zda≈Ñ")
print(f"{'='*80}")
print(f"Tokenizacja {len(raw_sentences):,} zda≈Ñ...")
print(f"To mo≈ºe chwilƒô potrwaƒá...")

# encode_batch() przetwarza wszystkie zdania r√≥wnolegle - du≈ºo szybsze!
encodings = tokenizer.encode_batch(raw_sentences)

# Konwersja obiekt√≥w Encoding na listƒô list string√≥w (token√≥w)
# Ka≈ºde zdanie staje siƒô listƒÖ token√≥w
# Przyk≈Çad: "Litwo! Ojczyzno moja!" ‚Üí ["Litwo", "!", "Ojczy", "zno", "moja", "!"]
tokenized_sentences = [
    encoding.tokens for encoding in encodings
]

print(f"‚úì Przygotowano {len(tokenized_sentences):,} sekwencji token√≥w")

# Statystyki tokenizacji
total_tokens = sum(len(sent) for sent in tokenized_sentences)
avg_tokens = total_tokens / len(tokenized_sentences) if tokenized_sentences else 0
print(f"‚úì ≈ÅƒÖczna liczba token√≥w: {total_tokens:,}")
print(f"‚úì ≈örednia d≈Çugo≈õƒá zdania: {avg_tokens:.1f} token√≥w")

# Przyk≈Çad tokenizacji (pierwsze 3 zdania)
print(f"\nPrzyk≈Çad tokenizacji (pierwsze 3 zdania):")
for i, sent in enumerate(tokenized_sentences[:3], 1):
    print(f"  Zdanie {i} ({len(sent)} token√≥w): {sent[:15]}{'...' if len(sent) > 15 else ''}")

# --- ETAP 2: Trening Word2Vec (CBOW) ---

print(f"\n{'='*80}")
print(f"KROK 4: Trening modelu Word2Vec (CBOW)")
print(f"{'='*80}")
print(f"Parametry treningu:")
print(f"  ‚Ä¢ Wymiar wektora (vector_size): {VECTOR_LENGTH}")
print(f"  ‚Ä¢ Rozmiar okna (window): {WINDOW_SIZE}")
print(f"  ‚Ä¢ Min. czƒôsto≈õƒá (min_count): {MIN_COUNT}")
print(f"  ‚Ä¢ Liczba epok (epochs): {EPOCHS}")
print(f"  ‚Ä¢ Tryb: {'CBOW' if SG_MODE == 0 else 'Skip-gram'}")
print(f"  ‚Ä¢ WƒÖtki (workers): {WORKERS}")
print(f"\nUruchamiam trening...")
print(f"{'='*80}\n")

"""
Jak dzia≈Ça CBOW (Continuous Bag-of-Words):

1. Dla ka≈ºdego s≈Çowa w zdaniu:
   - Bierze WINDOW_SIZE s≈Ç√≥w z lewej strony
   - Bierze WINDOW_SIZE s≈Ç√≥w z prawej strony
   - Pr√≥buje przewidzieƒá s≈Çowo ≈õrodkowe na podstawie kontekstu

2. Przyk≈Çad dla WINDOW_SIZE=2:
   Zdanie: "kr√≥l siedzia≈Ç na tronie w zamku"
   Cel: przewidzieƒá "tronie"
   Input: ["kr√≥l", "siedzia≈Ç", "na"] + ["w", "zamku"] ‚Üí Output: "tronie"

3. Sieƒá neuronowa:
   Input Layer ‚Üí Hidden Layer (embedding) ‚Üí Output Layer
   
4. Wagi hidden layer to w≈Ça≈õnie nasze embeddingi!
   Ka≈ºdy token dostaje wektor o d≈Çugo≈õci VECTOR_LENGTH

5. Trening przez EPOCHS epok:
   - Ka≈ºda epoka = jedno przej≈õcie przez ca≈Çy korpus
   - Wagi sƒÖ stopniowo dostosowywane aby lepiej przewidywaƒá s≈Çowa
   - Efekt: podobne semantycznie s≈Çowa majƒÖ podobne wektory
"""

model = Word2Vec(
    sentences=tokenized_sentences,  # Dane treningowe (lista list token√≥w)
    vector_size=VECTOR_LENGTH,      # Wymiar wektora embeddingowego
    window=WINDOW_SIZE,             # Rozmiar okna kontekstowego
    min_count=MIN_COUNT,            # Minimalna czƒôsto≈õƒá tokenu
    workers=WORKERS,                # Liczba wƒÖtk√≥w
    sg=SG_MODE,                     # 0: CBOW, 1: Skip-gram
    epochs=EPOCHS,                  # Liczba epok treningu
    sample=SAMPLE_RATE,             # Downsampling czƒôstych s≈Ç√≥w
)

print(f"\n{'='*80}")
print(f"‚úì Trening zako≈Ñczony pomy≈õlnie!")
print(f"{'='*80}")
print(f"Statystyki modelu:")
print(f"  ‚Ä¢ Liczba unikalnych token√≥w w s≈Çowniku: {len(model.wv):,}")
print(f"  ‚Ä¢ Wymiar wektora: {model.wv.vector_size}")
print(f"  ‚Ä¢ ≈ÅƒÖczna liczba parametr√≥w: {len(model.wv) * model.wv.vector_size:,}")

# --- ETAP 3: Eksport i Zapis Wynik√≥w ---

print(f"\n{'='*80}")
print(f"KROK 5: Eksport wytrenowanego modelu")
print(f"{'='*80}")

"""
Eksportujemy model w 3 formatach:

1. Tensor NumPy (.npy):
   - Macierz wektor√≥w wszystkich token√≥w
   - Format: [num_tokens, vector_size]
   - U≈ºycie: szybkie ≈Çadowanie do NumPy/PyTorch/TensorFlow

2. Mapa token‚Üíindeks (.json):
   - S≈Çownik mapujƒÖcy tokeny na ich indeksy w tensorze
   - Format: {"token": index, ...}
   - U≈ºycie: translacja tokenu na indeks w tensorze

3. Pe≈Çny model Gensim (.model):
   - Zawiera wszystko: wektory, s≈Çownik, metadane
   - Format: w≈Ça≈õciwy dla gensim
   - U≈ºycie: kontynuacja treningu, operacje na wektorach
"""

# 1. EKSPORT TENSORA NUMPY
# Pobieramy macierz wszystkich wektor√≥w z modelu
embedding_matrix_np = model.wv.vectors  # Shape: (vocab_size, vector_size)
embedding_matrix_tensor = np.array(embedding_matrix_np, dtype=np.float32)

print(f"\nTensor embeddingowy:")
print(f"  ‚Ä¢ Kszta≈Çt: {embedding_matrix_tensor.shape} (Tokeny √ó Wymiar)")
print(f"  ‚Ä¢ Typ danych: {embedding_matrix_tensor.dtype}")
print(f"  ‚Ä¢ Rozmiar w pamiƒôci: {embedding_matrix_tensor.nbytes / 1024 / 1024:.2f} MB")

np.save(OUTPUT_TENSOR_FILE, embedding_matrix_tensor)
print(f"  ‚úì Zapisano jako: '{OUTPUT_TENSOR_FILE}'")

# 2. EKSPORT MAPOWANIA TOKEN‚ÜíINDEKS
# Ka≈ºdy token ma sw√≥j unikalny indeks w tensorze
# Przyk≈Çad: {"Litwo": 0, "Ojczy": 1, "zno": 2, ...}
token_to_index = {token: model.wv.get_index(token) for token in model.wv.index_to_key}

print(f"\nMapa token√≥w:")
print(f"  ‚Ä¢ Liczba token√≥w: {len(token_to_index):,}")
print(f"  ‚Ä¢ Przyk≈Çadowe tokeny: {list(token_to_index.keys())[:5]}")

with open(OUTPUT_MAP_FILE, "w", encoding="utf-8") as f:
    json.dump(token_to_index, f, ensure_ascii=False, indent=2)
print(f"  ‚úì Zapisano jako: '{OUTPUT_MAP_FILE}'")

# 3. EKSPORT PE≈ÅNEGO MODELU GENSIM
# Zawiera wszystko - mo≈ºna wczytaƒá i kontynuowaƒá trening lub u≈ºywaƒá API gensim
model.save(OUTPUT_MODEL_FILE)
print(f"\nPe≈Çny model:")
print(f"  ‚úì Zapisano jako: '{OUTPUT_MODEL_FILE}'")

print(f"\n{'='*80}")
print(f"‚úì Wszystkie pliki zapisane pomy≈õlnie!")
print(f"{'='*80}")

# --- ETAP 4: FUNKCJE POMOCNICZE DO TESTOWANIA EMBEDDINGU ---

def get_word_vector_and_similar(word: str, tokenizer: Tokenizer, model: Word2Vec, topn: int = 20, filter_noise: bool = True):
    """
    Oblicza wektor embeddingowy dla ca≈Çego s≈Çowa i znajduje najbardziej podobne tokeny.
    
    Proces:
    1. Tokenizuje s≈Çowo na podwyrazowe tokeny (np. "wojsko" ‚Üí ["woj", "sko"])
    2. Pobiera wektory dla ka≈ºdego tokenu z modelu
    3. U≈õrednia wektory token√≥w ‚Üí wektor s≈Çowa
    4. Znajduje najbardziej podobne tokeny u≈ºywajƒÖc podobie≈Ñstwa kosinusowego
    5. [Opcjonalnie] Filtruje szumy (same spacje, uszkodzone znaki)
    
    Args:
        word (str): S≈Çowo do analizy (np. "wojsko", "szlachta")
        tokenizer (Tokenizer): Tokenizer BPE do podzia≈Çu s≈Çowa na tokeny
        model (Word2Vec): Wytrenowany model Word2Vec z embeddingami
        topn (int): Ile najbardziej podobnych token√≥w zwr√≥ciƒá (domy≈õlnie 20)
        filter_noise (bool): Czy filtrowaƒá "szumy" z wynik√≥w (domy≈õlnie True)
                            Usuwa: same spacje (ƒ†), uszkodzone Unicode, tokeny specjalne
        
    Returns:
        tuple: (word_vector, similar_tokens) gdzie:
            - word_vector: np.array - u≈õredniony wektor s≈Çowa (shape: [vector_size])
            - similar_tokens: list[(str, float)] - lista (token, similarity_score)
            
        lub (None, None) je≈õli nie mo≈ºna obliczyƒá wektora
        
    Podobie≈Ñstwo kosinusowe:
        similarity = (A ¬∑ B) / (||A|| √ó ||B||)
        Zakres: [-1, 1] gdzie:
            1.0 = identyczne kierunki (bardzo podobne)
            0.0 = prostopad≈Çe (niezwiƒÖzane)
           -1.0 = przeciwne kierunki (przeciwstawne)
           
    Przyk≈Çad:
        >>> vector, similar = get_word_vector_and_similar("kr√≥l", tokenizer, model, topn=5)
        >>> # vector: array([0.123, -0.456, 0.789, ...])
        >>> # similar: [("ksiƒÖ≈ºƒô", 0.721), ("w≈Çadca", 0.689), ...]
        
    Uwagi:
        - S≈Çowo musi zawieraƒá przynajmniej jeden token znany modelowi
        - Rzadkie s≈Çowa (< MIN_COUNT) mogƒÖ nie mieƒá wektor√≥w
        - Dla lepszych wynik√≥w u≈ºywaj s≈Ç√≥w z korpusu treningowego
    """
    # KROK 1: Tokenizacja s≈Çowa
    # Dodajemy spacje aby tokenizer widzia≈Ç s≈Çowo w kontek≈õcie (wa≈ºne dla token√≥w ze spacjƒÖ)
    encoding = tokenizer.encode(" " + word + " ") 
    word_tokens = [t.strip() for t in encoding.tokens if t.strip()]  # Usu≈Ñ puste tokeny
    
    # KROK 2: Czyszczenie token√≥w specjalnych
    # Usuwamy tokeny poczƒÖtku/ko≈Ñca sekwencji je≈õli zosta≈Çy dodane
    if word_tokens and word_tokens[0] in ['[CLS]', '<s>', 'ƒ†']:
        word_tokens = word_tokens[1:]
    if word_tokens and word_tokens[-1] in ['[SEP]', '</s>']:
        word_tokens = word_tokens[:-1]

    valid_vectors = []
    missing_tokens = []
    
    # KROK 3: Zbieranie wektor√≥w dla ka≈ºdego tokenu
    for token in word_tokens:
        if token in model.wv:
            # Token znaleziony w modelu - pobierz jego wektor
            valid_vectors.append(model.wv[token])
        else:
            # Token zbyt rzadki (< MIN_COUNT) lub nieznany
            missing_tokens.append(token)

    # KROK 4: Sprawdzenie czy mamy jakiekolwiek wektory
    if not valid_vectors:
        if missing_tokens:
            print(f"‚úó S≈Çowo '{word}' ‚Üí tokeny {word_tokens}")
            print(f"  ≈ªaden token nie znajduje siƒô w s≈Çowniku (MIN_COUNT={MIN_COUNT})")
        else:
            print(f"‚úó S≈Çowo '{word}' nie zosta≈Ço przetokenizowane poprawnie")
        return None, None

    # KROK 5: U≈õrednianie wektor√≥w
    # Wektor ca≈Çego s≈Çowa = ≈õrednia wektor√≥w jego token√≥w sk≈Çadowych
    # Przyk≈Çad: "wojsko" = ["woj", "sko"] ‚Üí ≈õrednia(vec("woj"), vec("sko"))
    word_vector = np.mean(valid_vectors, axis=0)

    # KROK 6: Znalezienie najbardziej podobnych token√≥w
    # U≈ºywamy podobie≈Ñstwa kosinusowego miƒôdzy word_vector a wszystkimi wektorami w modelu
    # Pobieramy wiƒôcej wynik√≥w je≈õli bƒôdziemy filtrowaƒá
    fetch_count = topn * 3 if filter_noise else topn
    
    similar_words = model.wv.most_similar(
        positive=[word_vector],  # Szukamy token√≥w podobnych do tego wektora
        topn=fetch_count         # Zwr√≥ƒá wiƒôcej aby m√≥c filtrowaƒá
    )
    
    # KROK 7: Filtrowanie szum√≥w (dla tokenizer√≥w GPT-2 style)
    if filter_noise:
        filtered_results = []
        
        for token, similarity in similar_words:
            # Pomi≈Ñ problematyczne tokeny
            skip = False
            
            # 1. Pomi≈Ñ sam prefix spacji (token "ƒ†" lub " ")
            if token in ['ƒ†', ' ', '‚ñÅ']:
                skip = True
            
            # 2. Pomi≈Ñ tokeny zawierajƒÖce uszkodzone znaki Unicode
            # GPT-2 byte-level BPE: ≈Ç‚Üí√Öƒ§, ≈Ñ‚Üí√Öƒ¶, √≥‚Üí√É¬≥, etc.
            if any(char in token for char in ['√É', '√Ñ', '√Ö', 'ƒÜ', 'ƒû']):
                skip = True
            
            # 3. Pomi≈Ñ tokeny specjalne
            if token in ['[UNK]', '[CLS]', '[SEP]', '[PAD]', '[MASK]', '<s>', '</s>', '<unk>']:
                skip = True
            
            # 4. Pomi≈Ñ bardzo kr√≥tkie tokeny (czƒôsto artefakty)
            if len(token.strip('ƒ†‚ñÅ')) <= 1:
                skip = True
            
            if not skip:
                filtered_results.append((token, similarity))
            
            # Przerwij gdy mamy wystarczajƒÖco wynik√≥w
            if len(filtered_results) >= topn:
                break
        
        similar_words = filtered_results
    
    return word_vector, similar_words

# --- ETAP 5: WERYFIKACJA I TESTOWANIE EMBEDDINGU ---

print(f"\n{'='*80}")
print(f"KROK 6: Weryfikacja jako≈õci embeddingu")
print(f"{'='*80}")
print(f"Test: Szukanie semantycznie podobnych s≈Ç√≥w")
print(f"Metoda: U≈õrednianie wektor√≥w token√≥w sk≈Çadowych")
print(f"{'='*80}\n")

"""
Jak testujemy jako≈õƒá embeddingu?

1. Podobie≈Ñstwo semantyczne:
   - S≈Çowa o podobnym znaczeniu powinny mieƒá podobne wektory
   - Np. "kr√≥l" ‚âà "ksiƒÖ≈ºƒô", "wojsko" ‚âà "armia"

2. Analogie wektorowe:
   - kr√≥l - mƒô≈ºczyzna + kobieta ‚âà kr√≥lowa
   - dziecko + kobieta ‚âà dziewczyna/c√≥rka

3. Co oznacza dobry wynik?
   - Similarity > 0.7: Bardzo dobre podobie≈Ñstwo
   - Similarity 0.5-0.7: Dobre podobie≈Ñstwo
   - Similarity < 0.5: S≈Çabe podobie≈Ñstwo
   
4. Cele optymalizacji (z Zadania 4.1):
   - kr√≥l-ksiƒÖ≈ºƒô: jak najbli≈ºej 1.0
   - kobieta-dziewczyna: jak najbli≈ºej 1.0
   - Zwiƒôksz EPOCHS, VECTOR_LENGTH, lub WINDOW_SIZE je≈õli wyniki s≈Çabe

UWAGA DLA TOKENIZERA GPT2-POLISH:
   - GPT-2 u≈ºywa byte-level BPE z prefiksem 'ƒ†' dla spacji
   - "szlachta" tokenizuje siƒô jako ['sz', 'lach', 'ta'] (BEZ spacji)
   - Ale w korpusie mo≈ºe byƒá "ƒ†szlachta" (ZE spacjƒÖ) - to INNY token!
   - Filtrowanie szum√≥w usuwa: ƒ† (samƒÖ spacjƒô), uszkodzone Unicode (√Öƒ§, √É¬≥)
"""

# Testowe s≈Çowa - powinny mieƒá sensowne odpowiedniki semantyczne
words_to_test = ['wojsko', 'szlachta', 'choroba', 'kr√≥l'] 

for word in words_to_test:
    word_vector, similar_tokens = get_word_vector_and_similar(word, tokenizer, model, topn=10)
    
    if word_vector is not None:
        # Poka≈º jak s≈Çowo zosta≈Ço stokenizowane
        tokens_used = tokenizer.encode(word).tokens
        
        print(f"{'‚îÄ'*80}")
        print(f"üîç Analiza s≈Çowa: '{word}'")
        print(f"{'‚îÄ'*80}")
        print(f"Tokenizacja: {tokens_used}")
        print(f"Wektor (pierwsze 10 wymiar√≥w): {word_vector[:10]}")
        print(f"\nüéØ Top 10 najbardziej podobnych token√≥w:")
        
        for i, (token, similarity) in enumerate(similar_tokens, 1):
            # Dodaj emoji dla r√≥≈ºnych poziom√≥w podobie≈Ñstwa
            if similarity > 0.7:
                emoji = "üî•"  # Bardzo podobne
            elif similarity > 0.6:
                emoji = "‚ú®"  # Podobne
            elif similarity > 0.5:
                emoji = "‚úì"   # Do≈õƒá podobne
            else:
                emoji = "‚óã"   # S≈Çabo podobne
                
            print(f"  {i:2d}. {emoji} {token:20s} ‚Üí similarity: {similarity:.4f}")
        print()

# --- TEST ANALOGII WEKTOROWYCH ---

print(f"\n{'='*80}")
print(f"KROK 7: Test analogii wektorowych")
print(f"{'='*80}")
print(f"Wz√≥r: token1 + token2 ‚Üí znajd≈∫ najbardziej podobny wynik")
print(f"Interpretacja: Jaki token ≈ÇƒÖczy cechy obu token√≥w?")
print(f"{'='*80}\n")

"""
Analogie wektorowe - matematyka embedding√≥w:

Je≈õli mamy dobrze wytrenowany embedding:
- vec("kr√≥l") - vec("mƒô≈ºczyzna") + vec("kobieta") ‚âà vec("kr√≥lowa")
- vec("dziecko") + vec("kobieta") ‚âà vec("dziewczyna")

W praktyce:
1. Dodajemy wektory dw√≥ch s≈Ç√≥w
2. Szukamy token√≥w najbli≈ºszych tej sumie
3. Je≈õli wynik ma sens semantycznie = dobry embedding!
"""

# Para token√≥w do analogii
tokens_analogy = ['dziecko', 'kobieta']

# Sprawdzamy czy oba tokeny istniejƒÖ w modelu
if tokens_analogy[0] in model.wv and tokens_analogy[1] in model.wv:
    print(f"üîç Analogia: '{tokens_analogy[0]}' + '{tokens_analogy[1]}'")
    print(f"Pytanie: Jaki token ≈ÇƒÖczy cechy obu s≈Ç√≥w?\n")
    
    similar_to_combined = model.wv.most_similar(
        positive=tokens_analogy,  # Suma wektor√≥w tych token√≥w
        topn=10                   # Top 10 wynik√≥w
    )

    print(f"üéØ Top 10 wynik√≥w:")
    for i, (token, similarity) in enumerate(similar_to_combined, 1):
        if similarity > 0.7:
            emoji = "üî•"
        elif similarity > 0.6:
            emoji = "‚ú®"
        else:
            emoji = "‚óã"
        print(f"  {i:2d}. {emoji} {token:20s} ‚Üí similarity: {similarity:.4f}")
    
    # Sprawd≈∫ czy oczekiwane s≈Çowa sƒÖ w wynikach
    expected_words = ['dziewczyna', 'c√≥rka', 'dziewczynka', 'matka']
    found = [word for word in expected_words if word in [t[0] for t in similar_to_combined]]
    
    if found:
        print(f"\n‚úì Znalezione oczekiwane s≈Çowa: {found}")
    else:
        print(f"\n‚ö† Brak oczekiwanych s≈Ç√≥w ({expected_words}) w top 10")
        print(f"  Wskaz√≥wka: Zwiƒôksz EPOCHS lub VECTOR_LENGTH dla lepszych wynik√≥w")
else:
    print(f"‚ö† OSTRZE≈ªENIE: Co najmniej jeden z token√≥w {tokens_analogy} nie znajduje siƒô w s≈Çowniku.")
    print(f"   Mo≈ºliwe przyczyny:")
    print(f"   - Token wystƒôpuje rzadziej ni≈º MIN_COUNT={MIN_COUNT}")
    print(f"   - Token nie wystƒôpuje w korpusie treningowym")
    print(f"   Pomijam test analogii.")

print(f"\n{'='*80}")
print(f"‚úì ZADANIE 4.1 UKO≈ÉCZONE")
print(f"{'='*80}")
print(f"\nPliki wyj≈õciowe:")
print(f"  ‚Ä¢ {OUTPUT_TENSOR_FILE}")
print(f"  ‚Ä¢ {OUTPUT_MAP_FILE}")
print(f"  ‚Ä¢ {OUTPUT_MODEL_FILE}")
print(f"\nNastƒôpny krok:")
print(f"  Eksperymentuj z parametrami (VECTOR_LENGTH, EPOCHS, WINDOW_SIZE)")
print(f"  aby poprawiƒá jako≈õƒá embeddingu!")
print(f"{'='*80}")